{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic damage detection in Wikipedia\n",
    "This notebook demonstrates the basic contruction of a vandalism classification system using [revscoring](http://pythonhosted.org/revscoring/).\n",
    "\n",
    "The basic process that we'll follow is this:\n",
    "\n",
    "1. Gather example of human judgement applied to Wikipedia edits.  In this case, we'll take advantage of [reverts](https://meta.wikimedia.org/wiki/Research:Revert).  \n",
    "2. Split the data into a training and testing set\n",
    "3. Training the machine learning model\n",
    "4. Testing the machine learning model\n",
    "\n",
    "And then we'll have some fun applying the model to some edits using RCStream.  The following diagram gives a good sense for the whole process.\n",
    "\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Supervised_machine_learning_in_a_nutshell.svg/640px-Supervised_machine_learning_in_a_nutshell.svg.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Getting labeled observations\n",
    "<img style=\"float: right; margin: 1ex;\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Machine_learning_nutshell_--_Gather_labeled_observations.svg/300px-Machine_learning_nutshell_--_Gather_labeled_observations.svg.png\" />\n",
    "\n",
    "Regretfully, running SQL queries isn't something we can do directly from the notebook *yet*.  So, we'll use [Quarry](https://quarry.wmflabs.org) to generate a nice random sample of edits.  20,000 observations should do just fine.  Here's the query I want to run:\n",
    "\n",
    "```SQL\n",
    "USE enwiki_p;\n",
    "SELECT rev_id \n",
    "FROM revision \n",
    "WHERE rev_timestamp BETWEEN \"20150201\" AND \"20160201\" \n",
    "ORDER BY RAND() \n",
    "LIMIT 20000;\n",
    "```\n",
    "\n",
    "See http://quarry.wmflabs.org/query/7530.  By clicking around the UI, I can see that this URL will download my tab-separated file: http://quarry.wmflabs.org/run/65415/output/0/tsv?download=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Magical ipython notebook stuff puts the result of this command into a variable\n",
    "revids_f = !wget http://quarry.wmflabs.org/run/65415/output/0/tsv?download=true -qO- \n",
    "\n",
    "revids = [int(line) for line in revids_f[1:]]\n",
    "len(revids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK.  Now that we have a set of revisions, we need to label them.  In this case, we're going to label them as reverted/not.  We want to exclude a few different types of reverts -- e.g. when a user reverts themself or when an edit is reverted back to by someone else.  For this, we'll use the [mwreverts](https://pythonhosted.org/mwreverts) and [mwapi](https://pythonhosted.org/mwapi) libraries.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...............r...."
     ]
    }
   ],
   "source": [
    "import sys, traceback\n",
    "import mwreverts.api\n",
    "import mwapi\n",
    "\n",
    "# We'll use the mwreverts API check.  In order to do that, we need an API session\n",
    "session = mwapi.Session(\"https://en.wikipedia.org\", \n",
    "                        user_agent=\"Revert detection demo <ahalfaker@wikimedia.org>\")\n",
    "\n",
    "# For each revision, find out if it was \"reverted\" and label it so.\n",
    "rev_reverteds = []\n",
    "for rev_id in revids[:20]:  # TODO: Limiting to the first 20!!!!\n",
    "    try:\n",
    "        _, reverted, reverted_to = mwreverts.api.check(\n",
    "            session, rev_id, radius=5,  # most reverts within 5 edits\n",
    "            window=48*60*60,  # 2 days\n",
    "            rvprop={'user', 'ids'})  # Some properties we'll make use of\n",
    "    except RuntimeError as e:\n",
    "        sys.stderr.write(str(e))\n",
    "        continue\n",
    "    \n",
    "    if reverted is not None:\n",
    "        reverted_doc = [r for r in reverted.reverteds\n",
    "                        if r['revid'] == rev_id][0]\n",
    "\n",
    "        # self-reverts\n",
    "        self_revert = \\\n",
    "            reverted_doc['user'] == reverted.reverting['user']\n",
    "        \n",
    "        # revisions that are reverted back to by others\n",
    "        reverted_back_to = \\\n",
    "            reverted_to is not None and \\\n",
    "            reverted_doc['user'] != \\\n",
    "            reverted_to.reverting['user']\n",
    "        \n",
    "        # If we are reverted, not by self or reverted back to by someone else, \n",
    "        # then, let's assume it was damaging.\n",
    "        damaging_reverted = !(self_revert or reverted_back_to)\n",
    "    else:\n",
    "        damaging_reverted = False\n",
    "\n",
    "    rev_reverteds.append((rev_id, damaging_reverted))\n",
    "    sys.stderr.write(\"r\" if damaging_reverted else \".\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eeek!  This takes too long.  You get the idea.  So, I uploaded dataset that has already been labeled here @ `../datasets/demo/enwiki.rev_reverted.20k_2015.tsv.bz2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19868"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_reverteds_f = !bzcat ../datasets/demo/enwiki.rev_reverted.20k_2015.tsv.bz2\n",
    "rev_reverteds = [line.strip().split(\"\\t\") for line in rev_reverteds_f[1:]]\n",
    "rev_reverteds = [(int(rev_id), reverted == \"True\") for rev_id, reverted in rev_reverteds]\n",
    "len(rev_reverteds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK.  It looks like we got an error when trying to extract the reverted status of ~132 edits, which is an acceptable loss.  Now just to make sure we haven't gone crazy, let's check some of the reverted edits:\n",
    "\n",
    "* https://en.wikipedia.org/wiki/?diff=695071713 (section blanking)\n",
    "* https://en.wikipedia.org/wiki/?diff=667375206 (unexplained addition of nonsense)\n",
    "* https://en.wikipedia.org/wiki/?diff=670204366 (vandalism \"I don't know\")\n",
    "* https://en.wikipedia.org/wiki/?diff=680329354 (adds non-existent category)\n",
    "* https://en.wikipedia.org/wiki/?diff=668682186 (test edit -- removes punctuation)\n",
    "* https://en.wikipedia.org/wiki/?diff=666882037 (adds spamlink)\n",
    "* https://en.wikipedia.org/wiki/?diff=663302354 (adds nonsense special char)\n",
    "* https://en.wikipedia.org/wiki/?diff=675803278 (unconstructive link changes)\n",
    "* https://en.wikipedia.org/wiki/?diff=680203994 (vandalism -- \"Pepe meme\")\n",
    "* https://en.wikipedia.org/wiki/?diff=656734057 (\"JELENAS BOOTY UNDSO\")\n",
    "\n",
    "OK.  Looks like we are doing pretty good. :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Split the data into a training and testing set\n",
    "<img style=\"float: right; margin: 1ex;\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Machine_learning_nutshell_--_Split_into_train-test_set.svg/320px-Machine_learning_nutshell_--_Split_into_train-test_set.svg.png\" />\n",
    "Before we move on with training, it's important that we hold back some of the data for testing later.  If we train on the same data we'll test with, we risk [overfitting](https://en.wikipedia.org/wiki/Overfitting) and not noticing!\n",
    "\n",
    "In this section, we'll both split the training and testing set *and* gather prective features for each of the labeled observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 4868)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = rev_reverteds[:15000]\n",
    "test_set = rev_reverteds[15000:]\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK.  In order to train the machine learning model, we'll need to give it a source of signal.  This is where \"features\" come into play.  A feature represents a simple numerical statistic that we can extract from our observations that we think will be *predictive* of our outcome.  Luckily, `revscoring` provides a whole suite of features that work well for damage detection.  In this case, we'll be looking at features of the edit diff.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from revscoring.features import wikitext, revision_oriented, temporal\n",
    "from revscoring.languages import english\n",
    "\n",
    "features = [\n",
    "    # Catches long key mashes like kkkkkkkkkkkk\n",
    "    wikitext.revision.diff.longest_repeated_char_added,\n",
    "    # Measures the size of the change in added words\n",
    "    wikitext.revision.diff.words_added,\n",
    "    # Measures the size of the change in removed words\n",
    "    wikitext.revision.diff.words_removed,\n",
    "    # Measures the proportional change in \"badwords\"\n",
    "    english.badwords.revision.diff.match_prop_delta_sum,\n",
    "    # Measures the proportional change in \"informals\"\n",
    "    english.informals.revision.diff.match_prop_delta_sum,\n",
    "    # Measures the proportional change meaningful words\n",
    "    english.stopwords.revision.diff.non_stopword_prop_delta_sum,\n",
    "    # Is the user anonymous\n",
    "    revision_oriented.revision.user.is_anon,\n",
    "    # Is the user a bot or a sysop\n",
    "    revision_oriented.revision.user.in_group({'bot', 'sysop'}),\n",
    "    # How long ago did the user register?\n",
    "    temporal.revision.user.seconds_since_registration\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll need to turn to `revscoring`s feature extractor to help us get us feature values for each revision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "695071713 [1, 0, 10974, -1.0, -2.5476190476190474, -1477.9699604325442, True, False, 313948852]\n",
      "667375206 [1, 1, 1, 0.0, 0.0, 0.33333333333333337, False, False, 9844289]\n"
     ]
    }
   ],
   "source": [
    "from revscoring.extractors import api\n",
    "api_extractor = api.Extractor(session)\n",
    "\n",
    "print(695071713, list(api_extractor.extract(695071713, features)))\n",
    "print(667375206, list(api_extractor.extract(667375206, features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...................."
     ]
    }
   ],
   "source": [
    "# Now for the whole set!\n",
    "training_features_reverted = []\n",
    "for rev_id, reverted in train_set[:20]:\n",
    "    try:\n",
    "        feature_values = list(api_extractor.extract(rev_id, features))\n",
    "    except RuntimeError as e:\n",
    "        sys.stderr.write(str(e))\n",
    "        continue\n",
    "    \n",
    "    sys.stderr.write(\".\")\n",
    "    training_features_reverted.append((rev_id, feature_values, reverted))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eeek!  Again this takes too long, so again, I uploaded a dataset with features already extracted @ `../datasets/demo/enwiki.features_reverted.training.20k_2015.tsv.bz2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14979"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from revscoring.utilities.util import read_observations\n",
    "training_features_reverted_f = !bzcat ../datasets/demo/enwiki.features_reverted.training.20k_2015.tsv.bz2 | cut -f2-\n",
    "training_features_reverted = list(read_observations(training_features_reverted_f, features, lambda v: v==\"True\"))\n",
    "len(training_features_reverted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training the model\n",
    "<img style=\"float: right; margin: 1ex;\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Machine_learning_nutshell_--_Train_a_machine_learning_model.svg/320px-Machine_learning_nutshell_--_Train_a_machine_learning_model.svg.png\" />\n",
    "\n",
    "Now that we have a set of features extracted for our training set, it's time to train a model.  `revscoring` provides a set of different classifier algorithms.  From past experience, I know a [gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting) classifier works well, so we'll use that.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seconds_elapsed': 16.075700283050537}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from revscoring.scorer_models import GradientBoosting\n",
    "is_reverted = GradientBoosting(features, version=\"live demo!\", \n",
    "                               learning_rate=0.01, max_features=\"log2\", \n",
    "                               n_estimators=700, max_depth=5,\n",
    "                               balanced_sample_weight=True, scale=True, center=True)\n",
    "\n",
    "is_reverted.train(training_features_reverted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a trained model that we can play around with.  Let's try a few edits from our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True https://en.wikipedia.org/wiki/?diff=699665317 True 0.83\n",
      "True https://en.wikipedia.org/wiki/?diff=683832871 True 0.81\n",
      "True https://en.wikipedia.org/wiki/?diff=653913156 True 0.74\n",
      "True https://en.wikipedia.org/wiki/?diff=654545786 True 0.82\n",
      "True https://en.wikipedia.org/wiki/?diff=670608733 True 0.78\n",
      "True https://en.wikipedia.org/wiki/?diff=689399141 True 0.71\n",
      "True https://en.wikipedia.org/wiki/?diff=662365029 True 0.91\n",
      "True https://en.wikipedia.org/wiki/?diff=656782076 True 0.87\n",
      "True https://en.wikipedia.org/wiki/?diff=698954388 True 0.85\n",
      "True https://en.wikipedia.org/wiki/?diff=645603577 True 0.67\n",
      "False https://en.wikipedia.org/wiki/?diff=687073859 False 0.38\n",
      "False https://en.wikipedia.org/wiki/?diff=665341163 False 0.15\n",
      "False https://en.wikipedia.org/wiki/?diff=654524549 False 0.07\n",
      "False https://en.wikipedia.org/wiki/?diff=682425664 False 0.07\n",
      "False https://en.wikipedia.org/wiki/?diff=674780271 False 0.25\n",
      "False https://en.wikipedia.org/wiki/?diff=684793059 False 0.07\n",
      "False https://en.wikipedia.org/wiki/?diff=655583788 True 0.72\n",
      "False https://en.wikipedia.org/wiki/?diff=700003789 False 0.23\n",
      "False https://en.wikipedia.org/wiki/?diff=659306547 False 0.08\n",
      "False https://en.wikipedia.org/wiki/?diff=662149200 False 0.17\n"
     ]
    }
   ],
   "source": [
    "reverted_obs = [rev_id for rev_id, reverted in test_set if reverted]\n",
    "non_reverted_obs = [rev_id for rev_id, reverted in test_set if not reverted]\n",
    "\n",
    "for rev_id in reverted_obs[:10]:\n",
    "    feature_values = list(api_extractor.extract(rev_id, features))\n",
    "    score = is_reverted.score(feature_values)\n",
    "    print(True, \"https://en.wikipedia.org/wiki/?diff=\" + str(rev_id), \n",
    "          score['prediction'], round(score['probability'][True], 2))\n",
    "\n",
    "for rev_id in non_reverted_obs[:10]:\n",
    "    feature_values = list(api_extractor.extract(rev_id, features))\n",
    "    score = is_reverted.score(feature_values)\n",
    "    print(False, \"https://en.wikipedia.org/wiki/?diff=\" + str(rev_id), \n",
    "          score['prediction'], round(score['probability'][True], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Testing the model\n",
    "So, the above analysis can help give us a sense for whether the model is working or not, but it's hard to standardize between models.  So, we can apply some metrics that are specially crafted for machine learning models.  \n",
    "\n",
    "But first, I'll need to load the pre-generated feature values.  \n",
    "\n",
    "<center>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Machine_learning_nutshell_--_Test_the_machine_learning_model.svg/640px-Machine_learning_nutshell_--_Test_the_machine_learning_model.svg.png\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4862"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_features_reverted_f = !bzcat ../datasets/demo/enwiki.features_reverted.testing.20k_2015.tsv.bz2 | cut -f2-\n",
    "testing_features_reverted = list(read_observations(testing_features_reverted_f, features, lambda v: v==\"True\"))\n",
    "len(testing_features_reverted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision) -- The proportion of correct predictions\n",
    "* [Precision](https://en.wikipedia.org/wiki/Precision_and_recall) -- The proportion of correct positive predictions\n",
    "* [Recall](https://en.wikipedia.org/wiki/Precision_and_recall) -- The proportion of positive examples predicted as positive\n",
    "* [Receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) -- An information theoretic measure comparing the false-positive and true-positive rates across the prediction probability for a model. \n",
    "* Filter rate at 90% recall -- The proportion of observations that can be ignored while still catching 90% of \"reverted\" edits.  \n",
    "\n",
    "We'll use `revscoring` statistics to measure these against the test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScikitLearnClassifier\n",
      " - type: GradientBoosting\n",
      " - params: min_samples_leaf=1, min_weight_fraction_leaf=0.0, subsample=1.0, presort=\"auto\", learning_rate=0.01, center=true, warm_start=false, max_features=\"log2\", scale=true, max_leaf_nodes=null, random_state=null, verbose=0, init=null, loss=\"deviance\", min_samples_split=2, balanced_sample_weight=true, n_estimators=700, max_depth=5\n",
      " - version: live demo!\n",
      " - trained: 2016-02-24T12:57:11.355150\n",
      "\n",
      "Accuracy: 0.813\n",
      "Precision: 0.202\n",
      "Recall: 0.82\n",
      "ROC-AUC: 0.871\n",
      "Filter rate @ 0.9 recall: threshold=0.23, filter_rate=0.632, recall=0.902\n"
     ]
    }
   ],
   "source": [
    "from revscoring.scorer_models.test_statistics import (accuracy, precision, recall, \n",
    "                                                      roc, filter_rate_at_recall)\n",
    "\n",
    "is_reverted.test(testing_features_reverted, \n",
    "                 test_statistics=[accuracy(), precision(), recall(), roc(), \n",
    "                                  filter_rate_at_recall(0.90)])\n",
    "\n",
    "print(is_reverted.format_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus round!  Let's listen to Wikipedia's vandalism!\n",
    "\n",
    "So we don't have the most powerful damage detection classifier, but then again, we're only including 9 features.  Usually we run with ~60 features and get to much higher levels of fitness.  *but* this model is still useful and it should help us detect the most aggregious vandalism in Wikipedia.  In order to listen to Wikipedia, we'll need to connect to [RCStream](https://wikitech.wikimedia.org/wiki/RCStream) -- the same live feed that powers [listen to Wikipedia](http://listen.hatnote.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:socketIO_client:stream.wikimedia.org:80/socket.io/1: [packet error] unhandled namespace path ()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good edit https://en.wikipedia.org/wiki/?diff=706712470 0.25\n",
      "!!!Please review https://en.wikipedia.org/wiki/?diff=706712471 0.61\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712472 0.4\n",
      "!!!Please review https://en.wikipedia.org/wiki/?diff=706712473 0.65\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712474 0.17\n",
      "!!!Please review https://en.wikipedia.org/wiki/?diff=706712475 0.86\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712476 0.23\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712477 0.08\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712478 0.17\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712479 0.08\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712480 0.27\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712481 0.08\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712482 0.06\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712483 0.2\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712484 0.06\n",
      "!!!Please review https://en.wikipedia.org/wiki/?diff=706712485 0.64\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712486 0.11\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712487 0.19\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712488 0.34\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712489 0.04\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712490 0.16\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712491 0.05\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712492 0.12\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712493 0.12\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712494 0.15\n",
      "!!!Please review https://en.wikipedia.org/wiki/?diff=706712495 0.8\n",
      "!!!Please review https://en.wikipedia.org/wiki/?diff=706712496 0.55\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712497 0.27\n",
      "!!!Please review https://en.wikipedia.org/wiki/?diff=706712499 0.79\n",
      "!!!Please review https://en.wikipedia.org/wiki/?diff=706712498 0.79\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712500 0.24\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712501 0.16\n",
      "!!!Please review https://en.wikipedia.org/wiki/?diff=706712503 0.79\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712504 0.12\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712502 0.22\n",
      "!!!Please review https://en.wikipedia.org/wiki/?diff=706712505 0.8\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712506 0.23\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712507 0.13\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712509 0.24\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712510 0.03\n",
      "!!!Please review https://en.wikipedia.org/wiki/?diff=706712508 0.78\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712511 0.05\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712513 0.04\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712512 0.15\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712514 0.22\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712515 0.22\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712517 0.12\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712516 0.08\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712519 0.03\n",
      "Good edit https://en.wikipedia.org/wiki/?diff=706712518 0.14\n"
     ]
    }
   ],
   "source": [
    "import socketIO_client\n",
    "\n",
    "class WikiNamespace(socketIO_client.BaseNamespace):\n",
    "    def on_change(self, change):\n",
    "        if change['type'] not in ('new', 'edit'):\n",
    "            return\n",
    "        \n",
    "        rev_id = change['revision']['new']\n",
    "        feature_values = list(api_extractor.extract(rev_id, features))\n",
    "        score = is_reverted.score(feature_values)\n",
    "        if score['prediction']:\n",
    "            print(\"!!!Please review\", \"https://en.wikipedia.org/wiki/?diff=\" + str(rev_id), \n",
    "                  round(score['probability'][True], 2), flush=True)\n",
    "        else:\n",
    "            print(\"Good edit\", \"https://en.wikipedia.org/wiki/?diff=\" + str(rev_id),\n",
    "                  round(score['probability'][True], 2), flush=True)\n",
    "\n",
    "    def on_connect(self):\n",
    "        self.emit('subscribe', 'en.wikipedia.org')\n",
    "\n",
    "\n",
    "socketIO = socketIO_client.SocketIO('stream.wikimedia.org', 80)\n",
    "socketIO.define(WikiNamespace, '/rc')\n",
    "\n",
    "socketIO.wait(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
